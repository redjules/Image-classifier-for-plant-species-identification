{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpJLVYD9w7Q5"
   },
   "outputs": [],
   "source": [
    "# https://s3.amazonaws.com/hackerday.datascience/118/train.csv\n",
    "# https://s3.amazonaws.com/hackerday.datascience/118/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0LAreFzw7Q8"
   },
   "outputs": [],
   "source": [
    "# Image classification \n",
    "\n",
    "# Get all the images\n",
    "\n",
    "# Parse the images and store the pixels in structured data format\n",
    "\n",
    "# remove noise, if present\n",
    "\n",
    "# create models to learn the pattern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nu5JW9A7w7Q9"
   },
   "outputs": [],
   "source": [
    "# Tea leafs classification\n",
    "\n",
    "# The tea leafs comes through various cartons and with different batches\n",
    "\n",
    "# one individual does a manual inspection\n",
    "\n",
    "# then labeling happens by sampling exercise\n",
    "\n",
    "# High, medium and low quality of leafs that comes to the manufacturer\n",
    "\n",
    "# different tea leafs fetch a differential pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSKAU0ARw7Q9"
   },
   "outputs": [],
   "source": [
    "    # Disease classification in plant leafs\n",
    "\n",
    "# each disease in plants have some unique pattern\n",
    "\n",
    "# can we classify them\n",
    "\n",
    "# can we predict them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gfdVDzQw7Q9"
   },
   "outputs": [],
   "source": [
    "# set of images\n",
    "\n",
    "# algorithms\n",
    "\n",
    "# prediction real time or batch mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1f9wZLHw7Q-"
   },
   "outputs": [],
   "source": [
    "# extraction of pixels and grey scale conversion, that translates the images into a structured data file\n",
    "\n",
    "# by two libraries in Python\n",
    "\n",
    "# scipy library\n",
    "\n",
    "# sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_4f2Co4w7Q-"
   },
   "outputs": [],
   "source": [
    "# we do not neeed to parse the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_q7A-Ihw7Q_"
   },
   "outputs": [],
   "source": [
    "# the data set stored in train and test file as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7_HhnmKw7Q_"
   },
   "outputs": [],
   "source": [
    "# training algorithms\n",
    "\n",
    "# Scalable training algorithms if the image is of high pixel size\n",
    "\n",
    "# base algorithms if the image is of low pixel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLJH9r76w7RA"
   },
   "outputs": [],
   "source": [
    "# for large pixel size and high dimensional data, CNN, convolutional neural network\n",
    "\n",
    "# First set of pixxels\n",
    "\n",
    "# apply convolutional layer\n",
    "\n",
    "# represent in another layer\n",
    "\n",
    "# apply another convolution\n",
    "\n",
    "# reduce the representation\n",
    "\n",
    "# apply convolution\n",
    "\n",
    "# manageable size and then train a neural network to solve this\n",
    "\n",
    "# tensorflow, pytorch, azure, GPU processing would be required, if the size of training set is huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aus2EtDAw7RA"
   },
   "outputs": [],
   "source": [
    "# if the pixel size is low\n",
    "\n",
    "# use algos from sklearn and scipy to perform classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGfGxxcUw7RA"
   },
   "outputs": [],
   "source": [
    "# keras: theano, tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iMH_b-9w7RB"
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEuTq23nw7RB"
   },
   "source": [
    "\n",
    "    train.csv - the training set\n",
    "    test.csv - the test set\n",
    "    sample_submission.csv - a sample submission file in the correct format\n",
    "    images - the image files (each image is named with its corresponding id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BVXv0yWw7RF"
   },
   "source": [
    "\n",
    "    id - an anonymous id unique to an image\n",
    "    margin_1, margin_2, margin_3, ..., margin_64 - each of the 64 attribute vectors for the margin feature\n",
    "    shape_1, shape_2, shape_3, ..., shape_64 - each of the 64 attribute vectors for the shape feature\n",
    "    texture_1, texture_2, texture_3, ..., texture_64 - each of the 64 attribute vectors for the texture feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0l9H4Jgw7RG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0xB-wGnw7RG"
   },
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs): pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtJK3vWow7RH"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPaztx6bw7RH"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('https://s3.amazonaws.com/hackerday.datascience/118/train.csv')\n",
    "test = pd.read_csv('https://s3.amazonaws.com/hackerday.datascience/118/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUxOyFR6w7RI"
   },
   "outputs": [],
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6JqzJgLw7RI"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTk1Ds4Ew7RJ"
   },
   "outputs": [],
   "source": [
    "train.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsVT5bcuw7RJ"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder().fit(train.species) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQns_blhw7RJ"
   },
   "outputs": [],
   "source": [
    "labels = le.transform(train.species) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oer99mH1w7RK"
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSmJpFBfw7RK"
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUzkSbEyw7RK"
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcUkyoHCw7RK"
   },
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "db9J14iEw7RL"
   },
   "outputs": [],
   "source": [
    "# a function to organize both training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYFam0JZw7RL"
   },
   "outputs": [],
   "source": [
    "def encode(train, test):\n",
    "    le = LabelEncoder().fit(train.species) \n",
    "    labels = le.transform(train.species)           # encode species strings\n",
    "    classes = list(le.classes_)                    \n",
    "    test_ids = test.id                             \n",
    "    \n",
    "    train = train.drop(['species', 'id'], axis=1)  \n",
    "    test = test.drop(['id'], axis=1)\n",
    "    \n",
    "    return train, labels, test, test_ids, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4rpXDU0w7RL"
   },
   "outputs": [],
   "source": [
    "train, labels, test, test_ids, classes = encode(train, test)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbTK6hOOw7RL"
   },
   "outputs": [],
   "source": [
    "# stratified sampling rather than simple random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaJbnTtmw7RM"
   },
   "outputs": [],
   "source": [
    "#Stratification is necessary for this dataset because there is a relatively large number of classes \n",
    "#(99 classes for 990 samples). This will ensure we have all classes represented in both the train and test indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ELcd5gSw7RM"
   },
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(10, test_size=0.2, random_state=1243)\n",
    "\n",
    "for train_index, test_index in sss.split(train, labels):\n",
    "    X_train, X_test = train.values[train_index], train.values[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "luhzyXxLw7RM"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hA9udH-lw7RM"
   },
   "outputs": [],
   "source": [
    "#selection of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-H_11sGHw7RN"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix, roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFwVo-XBw7RN"
   },
   "outputs": [],
   "source": [
    "# initialize all the classifiers\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    NuSVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "             min_impurity_decrease=0.0,\n",
    "             min_samples_leaf=1, min_samples_split=2,\n",
    "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "             oob_score=False, random_state=None, verbose=0,\n",
    "             warm_start=False),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKqrQWG0w7RO"
   },
   "outputs": [],
   "source": [
    "# initialize all the classifiers with the best parameters from grid search\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "            metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
    "            weights='uniform'),\n",
    "    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
    "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False),\n",
    "    NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n",
    "   decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
    "   max_iter=-1, nu=0.05, probability=True, random_state=None,\n",
    "   shrinking=True, tol=0.001, verbose=False),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "             min_impurity_decrease=0.0,\n",
    "             min_samples_leaf=1, min_samples_split=2,\n",
    "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "             oob_score=False, random_state=None, verbose=0,\n",
    "             warm_start=False),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDUXioJpw7RO"
   },
   "outputs": [],
   "source": [
    "classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MD9XdWg8w7RP"
   },
   "outputs": [],
   "source": [
    "# Logging for Visual Comparison\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NLZshJ6xjF8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GS05YOfFw7RQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "    train_predictions = clf.predict_proba(X_test)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    \n",
    "    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "    \n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCfRU0s_w7RQ"
   },
   "outputs": [],
   "source": [
    "# initialize all the classifiers\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    NuSVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NW_XJN-Cw7RQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit, GridSearchCV\n",
    "\n",
    "def fit_model(X,y):\n",
    "    cv_sets = ShuffleSplit(X_train.shape[0], test_size=0.20,random_state=1234)\n",
    "    sv = SVC()\n",
    "    params = {'kernel':('linear','poly','sigmoid','rbf'),\n",
    "              'C':[0.01,0.05,0.025,0.07,0.09,1.0]\n",
    "              }\n",
    "    grid = GridSearchCV(sv,params,cv=cv_sets)\n",
    "    grid = grid.fit(X,y)\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjJjwz0mw7RQ"
   },
   "outputs": [],
   "source": [
    "fit_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "di7qcPHcw7RR"
   },
   "outputs": [],
   "source": [
    "def fit_model(X,y):\n",
    "    cv_sets = ShuffleSplit(X_train.shape[0],test_size=0.20,random_state=1234)\n",
    "    nsv = NuSVC()\n",
    "    params = {'kernel':('linear','poly','sigmoid','rbf'),\n",
    "              'nu':[0.01,0.05,0.025]\n",
    "              }\n",
    "    grid = GridSearchCV(nsv,params,cv=cv_sets)\n",
    "    grid = grid.fit(X,y)\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ip-B8WsRw7RR"
   },
   "outputs": [],
   "source": [
    "fit_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4GxMW98w7RR"
   },
   "outputs": [],
   "source": [
    "def fit_model(X,y):\n",
    "    cv_sets = ShuffleSplit(X_train.shape[0],test_size=0.20,random_state=1234)\n",
    "    dt = DecisionTreeClassifier()\n",
    "    params = {'criterion':('gini','entropy'),\n",
    "              'max_depth':[2,3,4,5]\n",
    "              }\n",
    "    grid = GridSearchCV(dt,params,cv=cv_sets)\n",
    "    grid = grid.fit(X,y)\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLUxXnFNw7RR"
   },
   "outputs": [],
   "source": [
    "fit_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkaIQe_Ew7RS"
   },
   "outputs": [],
   "source": [
    "def fit_model(X,y):\n",
    "    cv_sets = ShuffleSplit(X_train.shape[0],test_size=0.20,random_state=1234)\n",
    "    rf = RandomForestClassifier()\n",
    "    params = {'criterion':('gini','entropy'),\n",
    "              'n_estimators':[100,200,300,500]\n",
    "              }\n",
    "    grid = GridSearchCV(rf,params,cv=cv_sets)\n",
    "    grid = grid.fit(X,y)\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZQlQTEyw7RS"
   },
   "outputs": [],
   "source": [
    "fit_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlMFr27Qw7RS"
   },
   "outputs": [],
   "source": [
    "def fit_model(X,y):\n",
    "    cv_sets = ShuffleSplit(X_train.shape[0],test_size=0.20,random_state=1234)\n",
    "    ab = AdaBoostClassifier()\n",
    "    params = {'algorithm':('SAMME', 'SAMME.R'),\n",
    "              'n_estimators':[100,200]\n",
    "              }\n",
    "    grid = GridSearchCV(ab,params,cv=cv_sets)\n",
    "    grid = grid.fit(X,y)\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEZiU_IIw7RY"
   },
   "outputs": [],
   "source": [
    "fit_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNPc3m1tw7RZ"
   },
   "outputs": [],
   "source": [
    "def fit_model(X,y):\n",
    "    cv_sets = ShuffleSplit(X_train.shape[0],test_size=0.20,random_state=1234)\n",
    "    gb = GradientBoostingClassifier()\n",
    "    params = {\n",
    "              'n_estimators':[100,200]\n",
    "              }\n",
    "    grid = GridSearchCV(gb,params,cv=cv_sets)\n",
    "    grid = grid.fit(X,y)\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0fVwMpLw7RZ"
   },
   "outputs": [],
   "source": [
    "fit_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-NWDM9Iw7RZ"
   },
   "outputs": [],
   "source": [
    "def fit_model(X,y):\n",
    "    cv_sets = ShuffleSplit(X_train.shape[0],test_size=0.20,random_state=1234)\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    params = {'solver' :('svd','eigen','lsqr')\n",
    "              }\n",
    "    grid = GridSearchCV(lda,params,cv=cv_sets)\n",
    "    grid = grid.fit(X,y)\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5G8UtNpw7Ra"
   },
   "outputs": [],
   "source": [
    "fit_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNaPfUchw7Rb"
   },
   "outputs": [],
   "source": [
    "# initialize all the classifiers with the best parameters from grid search\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "            metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
    "            weights='uniform'),\n",
    "    \n",
    "    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
    "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False),\n",
    "    \n",
    "    NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n",
    "   decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
    "   max_iter=-1, nu=0.05, probability=True, random_state=None,\n",
    "   shrinking=True, tol=0.001, verbose=False),\n",
    "    \n",
    "    DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, random_state=None,\n",
    "            splitter='best'),\n",
    "    \n",
    "    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False),\n",
    "    \n",
    "    AdaBoostClassifier(algorithm='SAMME', base_estimator=None, learning_rate=1.0,\n",
    "          n_estimators=200, random_state=None),\n",
    "    \n",
    "    GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
    "              max_features=None, max_leaf_nodes=None,\n",
    "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "              min_samples_leaf=1, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              random_state=None, subsample=1.0, verbose=0,\n",
    "              warm_start=False),\n",
    "    \n",
    "    GaussianNB(),\n",
    "    \n",
    "    LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
    "              solver='svd', store_covariance=False, tol=0.0001),\n",
    "    \n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6GQ1kxIw7Rb"
   },
   "outputs": [],
   "source": [
    "# Logging for Visual Comparison\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login to TrueFoundry  🎉\n",
    "\n",
    "1. An account with  <a href=\"https://projectpro.truefoundry.com/signin\">TrueFoundry</a>. has been created with the same email address that you use to sign in to ProjectPro and an email has been sent to you to set your password. \n",
    "2. Please go to your inbox and follow the link to make sure you are logged into TrueFoundry before getting to the next cell. If you don't see the email in your inbox, please check your Spam folder. \n",
    "\n",
    "Note: If you are not able to signin or did not receive an email, please send an email to nikunj@truefoundry.com with the following subject- \"ProjectPro User: TrueFoundry Login Issue\"!pip install mlfoundry --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbk2ZNDBw7Rb"
   },
   "outputs": [],
   "source": [
    "import mlfoundry as mlf\n",
    "\n",
    "TRACKING_URL = 'https://projectpro.truefoundry.com'\n",
    "mlf_api = mlf.get_client(TRACKING_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbhOjnRxw7Rb"
   },
   "outputs": [],
   "source": [
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    mlf_run = mlf_api.create_run(project_name='identify-plant-species-using-image-classifiers', run_name=name)\n",
    "    \n",
    "    #log the model and the parameters\n",
    "    mlf_run.log_model(clf, \"sklearn\")\n",
    "    mlf_run.log_params(clf.get_params())\n",
    "    \n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "    train_predictions = clf.predict_proba(X_test)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    \n",
    "    mlf_run.log_metrics({\"accuracy\":acc*100, \"log_loss\": ll})\n",
    "    \n",
    "    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "\n",
    "    try:\n",
    "        mat = confusion_matrix(y_test, clf.predict(X_test))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=mat)\n",
    "        disp.plot()\n",
    "        mlf_run.log_plots({\"ROC-curve\": plt}, step=1)\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUMEF4L5w7Rb"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"r\")\n",
    "plt.xlabel('Accuracy %')\n",
    "plt.title('Classifier Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJ9_HEPTw7Rb"
   },
   "outputs": [],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rm35Q0G9w7Rc"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Log Loss', y='Classifier', data=log, color=\"g\")\n",
    "plt.xlabel('Log Loss')\n",
    "plt.title('Classifier Log Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CKmKr1Tw7Rc"
   },
   "outputs": [],
   "source": [
    "# Predict Test Set\n",
    "favorite_clf = LinearDiscriminantAnalysis()\n",
    "favorite_clf.fit(X_train, y_train)\n",
    "test_predictions = favorite_clf.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMMZvJuQw7Rc"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEay6qAhw7Rc"
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whxEPurbw7Rc"
   },
   "outputs": [],
   "source": [
    "pred = favorite_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRm7Xzu7w7Rc"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5RrWn7zw7Rc"
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N83iD8ttw7Rc"
   },
   "outputs": [],
   "source": [
    "mlf_run = mlf_api.create_run(project_name='identify-plant-species-using-image-classifiers',\n",
    "                             run_name='Favourite-Classifier')\n",
    "\n",
    "#log the model and the parameters\n",
    "mlf_run.log_model(favorite_clf, \"sklearn\")\n",
    "mlf_run.log_params(favorite_clf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UD5MDKLszYPB"
   },
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_test,pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=mat)\n",
    "disp.plot()\n",
    "mlf_run.log_plots({\"ROC-curve\": plt}, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1J3-clDw7Rc"
   },
   "outputs": [],
   "source": [
    "test_data = np.hstack([X_test, np.reshape(y_test, (-1,1))])\n",
    "test_data = pd.DataFrame(test_data, columns= list(test.columns) + ['target'])\n",
    "test_data['prediction'] = favorite_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8NWo9Djw7Rd"
   },
   "outputs": [],
   "source": [
    "mlf_run.log_dataset(\n",
    "    dataset_name = 'train_dataset',\n",
    "    features = train_data[list(train.columns)],\n",
    "    predictions = train_data['predictions'],\n",
    "    actuals = train_data['targets'],\n",
    "    only_stats = False,   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGBldtqVw7Rd"
   },
   "outputs": [],
   "source": [
    "train_data = np.hstack([X_train, np.reshape(y_train, (-1,1))])\n",
    "train_data = pd.DataFrame(train_data, columns= list(train.columns) + ['target'])\n",
    "train_data['prediction'] = favorite_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INO_kZ4rw7Rd"
   },
   "outputs": [],
   "source": [
    "mlf_run.log_dataset(\n",
    "    dataset_name = 'test_dataset',\n",
    "    features = test_data[list(test.columns)],\n",
    "    predictions = test_data['predictions'],\n",
    "    actuals = test_data['targets'],\n",
    "    only_stats = False,   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqT65qtfzVa7"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9B5APAxw7Rd"
   },
   "source": [
    "# putting everything in grid search mode\n",
    "parameters =  \"insert params dict here\"\n",
    "\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring='log_loss')\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "clf = grid_obj.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dX1WL0Vmw7Rd"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np                     # numeric python lib\n",
    "\n",
    "import matplotlib.image as mpimg       # reading images to numpy arrays\n",
    "import matplotlib.pyplot as plt        # to plot any graph\n",
    "import matplotlib.patches as mpatches  # to draw a circle at the mean contour\n",
    "\n",
    "from skimage import measure            # to find shape contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLG4KWxLw7Rd"
   },
   "outputs": [],
   "source": [
    "import scipy.ndimage as ndi            # to determine shape centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiqJA_7dw7Rd"
   },
   "outputs": [],
   "source": [
    "# matplotlib setup\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (6, 6)      # setting default size of plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FepQ-ny_w7Rd"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKjJ2NX0w7Re"
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9e7Qocbw7Re"
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/pradmishra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N50osav-w7Re"
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvtPJDk1w7Re"
   },
   "outputs": [],
   "source": [
    "img = mpimg.imread('figure.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UN1xRgDKw7Re"
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCF1kobNw7Re"
   },
   "outputs": [],
   "source": [
    "# using image processing module of scipy to find the center of the leaf\n",
    "cx, cy, cz = ndi.center_of_mass(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTyN1-Trw7Re"
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cetPfOzdw7Re"
   },
   "outputs": [],
   "source": [
    "plt.imshow(img, cmap='Set3')  # show me the sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rN5Sfimqw7Rf"
   },
   "outputs": [],
   "source": [
    "plt.scatter(cx, cy)           # show me its center\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scuEXdk0w7Rf"
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "def fit_model(X,y):\n",
    "    cv_sets = ShuffleSplit(X_train.shape[0],n_iter=10,test_size=0.20,random_state=1234)\n",
    "    rf = RandomForestClassifier()\n",
    "    params = {'criterion':('gini','entropy'),\n",
    "              'max_depth':[2,3],\n",
    "              'min_samples_split':[30,10],\n",
    "             'max_features' : [4,5],\n",
    "             'n_estimators' : [50,100]}\n",
    "    grid = GridSearchCV(rf,params,cv=cv_sets)\n",
    "    grid = grid.fit(X,y)\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPJC66tQw7Rf"
   },
   "outputs": [],
   "source": [
    "fit_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4tHE9KJw7Rf"
   },
   "outputs": [],
   "source": [
    "nsimu = 21\n",
    "accuracy = [0]*nsimu\n",
    "ntree = [0]*nsimu\n",
    "for i in range(1,nsimu):\n",
    "    rf = RandomForestClassifier(n_estimators=i*5,min_samples_split=10,max_depth=None,criterion='gini')\n",
    "    rf.fit(X_train,y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,rf_pred)\n",
    "    accuracy[i] = (cm[0,0]+cm[1,1])/cm.sum()\n",
    "    ntree[i]=i*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUm4qzm6w7Rf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.scatter(x=ntree[1:nsimu],y=accuracy[1:nsimu],s=30,c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kXGFBPww7Rf"
   },
   "outputs": [],
   "source": [
    "nsimu = 21\n",
    "accuracy = [0]*nsimu\n",
    "ntree = [0]*nsimu\n",
    "for i in range(1,nsimu):\n",
    "    rf = RandomForestClassifier(n_estimators=i*5,min_samples_split=10,max_depth=None,criterion='entropy')\n",
    "    rf.fit(X_train,y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,rf_pred)\n",
    "    accuracy[i] = (cm[0,0]+cm[1,1])/cm.sum()\n",
    "    ntree[i]=i*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8_sbYP1w7Rf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.scatter(x=ntree[1:nsimu],y=accuracy[1:nsimu],s=30,c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "at6Qll-Iw7Rf"
   },
   "outputs": [],
   "source": [
    "nsimu = 21\n",
    "accuracy = [0]*nsimu\n",
    "ntree = [0]*nsimu\n",
    "for i in range(1,nsimu):\n",
    "    rf = RandomForestClassifier(n_estimators=i*5,min_samples_split=10,max_depth=10,criterion='entropy')\n",
    "    rf.fit(X_train,y_train)\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test,rf_pred)\n",
    "    accuracy[i] = (cm[0,0]+cm[1,1])/cm.sum()\n",
    "    ntree[i]=i*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKdF5Itlw7Rg"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.scatter(x=ntree[1:nsimu],y=accuracy[1:nsimu],s=30,c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3gTsC8tw7Rg"
   },
   "outputs": [],
   "source": [
    "def fit_model(X,y):\n",
    "    cv_sets = ShuffleSplit(X_train.shape[0],test_size=0.20,random_state=1234)\n",
    "    nnt = MLPClassifier()\n",
    "    params = {'activation':('relu','logistic'),\n",
    "              'hidden_layer_sizes':[2,3],\n",
    "              'solver':('sgd','adam','lbfgs'),\n",
    "             'max_iter' : [200,500],\n",
    "             'alpha' : [0.0001,0.001,0.01]}\n",
    "    grid = GridSearchCV(nnt,params,cv=cv_sets)\n",
    "    grid = grid.fit(X,y)\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrXrmpvnw7Rg"
   },
   "outputs": [],
   "source": [
    "#Next Step:\n",
    "# Grid search for hyper paramter tuning\n",
    "# TF - models\n",
    "# CNN- for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1KCSob1w7Rg"
   },
   "outputs": [],
   "source": [
    "## Importing standard libraries\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWNuPBxow7Rg"
   },
   "outputs": [],
   "source": [
    "#https://s3.amazonaws.com/hackerday.datascience/118/train.csv\n",
    "#https://s3.amazonaws.com/hackerday.datascience/118/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuJvPNWlw7Rg"
   },
   "outputs": [],
   "source": [
    "## Measure execution time, becaus Kaggle cloud fluctuates  \n",
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DS733hZnw7Rg"
   },
   "outputs": [],
   "source": [
    "## Importing sklearn libraries\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8bI8O_4w7Rg"
   },
   "outputs": [],
   "source": [
    "## Keras Libraries for Neural Networks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Merge\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBWEUmrPw7Rh"
   },
   "outputs": [],
   "source": [
    "## Read data from the CSV file\n",
    "data = pd.read_csv('https://s3.amazonaws.com/hackerday.datascience/118/train.csv')\n",
    "parent_data = data.copy()    ## Always a good idea to keep a copy of original data\n",
    "ID = data.pop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXditz4pw7Rh"
   },
   "outputs": [],
   "source": [
    "data.shape\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOcKu9RYw7Rh"
   },
   "outputs": [],
   "source": [
    "## Since the labels are textual, so we encode them categorically\n",
    "\n",
    "y = data.pop('species')\n",
    "y = LabelEncoder().fit(y).transform(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1GqfMKLw7Ro"
   },
   "outputs": [],
   "source": [
    "## Most of the learning algorithms are prone to feature scaling\n",
    "## Standardising the data to give zero mean =)\n",
    "from sklearn import preprocessing\n",
    "X = preprocessing.MinMaxScaler().fit(data).transform(data)\n",
    "X = StandardScaler().fit(data).transform(data)\n",
    "## normalizing does not help here; l1 and l2 allowed\n",
    "## X = preprocessing.normalize(data, norm='l1')\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICQafZzmw7Ro"
   },
   "outputs": [],
   "source": [
    "## We will be working with categorical crossentropy function\n",
    "## It is required to further convert the labels into \"one-hot\" representation\n",
    "\n",
    "y_cat = to_categorical(y)\n",
    "print(y_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8Tuq-Kyw7Ro"
   },
   "outputs": [],
   "source": [
    "## retain class balances\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.2,random_state=12345)\n",
    "train_index, val_index = next(iter(sss.split(X, y)))\n",
    "x_train, x_val = X[train_index], X[val_index]\n",
    "y_train, y_val = y_cat[train_index], y_cat[val_index]\n",
    "print(\"x_train dim: \",x_train.shape)\n",
    "print(\"x_val dim:   \",x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WcgyBzfw7Ro"
   },
   "outputs": [],
   "source": [
    "## Developing a layered model for Neural Networks\n",
    "## Input dimensions should be equal to the number of features\n",
    "## We used softmax layer to predict a uniform probabilistic distribution of outcomes\n",
    "## https://keras.io/initializations/ ;glorot_uniform, glorot_normal, lecun_uniform, orthogonal,he_normal\n",
    "# DNN- Deep Neural Network\n",
    "model = Sequential()\n",
    "model.add(Dense(768,input_dim=192,  kernel_initializer='glorot_normal', activation='tanh'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(768, activation='tanh'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(99, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIHkTpfBw7Ro"
   },
   "outputs": [],
   "source": [
    "## Error is measured as categorical crossentropy or multiclass logloss\n",
    "## Adagrad, rmsprop, SGD, Adadelta, Adam, Adamax, Nadam\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adagrad', metrics = [\"accuracy\"])\n",
    "#model.compile(loss='categorical_crossentropy',optimizer='sgd', metrics = [\"accuracy\"])\n",
    "#model.compile(loss='categorical_crossentropy',optimizer='adadelta', metrics = [\"accuracy\"])\n",
    "#model.compile(loss='categorical_crossentropy',optimizer='adam', metrics = [\"accuracy\"])\n",
    "#model.compile(loss='categorical_crossentropy',optimizer='adamax', metrics = [\"accuracy\"])\n",
    "#model.compile(loss='categorical_crossentropy',optimizer='Nadam', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHI5sWD4w7Ro"
   },
   "outputs": [],
   "source": [
    "## Fitting the model on the whole training data with early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "history = model.fit(x_train, y_train,batch_size=192,nb_epoch=250 ,verbose=0,\n",
    "                    validation_data=(x_val, y_val),callbacks=[early_stopping])\n",
    "\n",
    "mlf_run = mlf_api.create_run(project_name='identify-plant-species-using-image-classifiers',\n",
    "                             run_name='Keras-Model')\n",
    "mlf_run.log_model(model, \"keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9UMHu5Fw7Ro"
   },
   "outputs": [],
   "source": [
    "## we need to consider the loss for final submission to leaderboard\n",
    "## print(history.history.keys())\n",
    "print('val_acc: ',max(history.history['val_acc']))\n",
    "print('val_loss: ',min(history.history['val_loss']))\n",
    "print('train_acc: ',max(history.history['acc']))\n",
    "print('train_loss: ',min(history.history['loss']))\n",
    "\n",
    "print()\n",
    "print(\"train/val loss ratio: \", min(history.history['loss'])/min(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSA3NtEFw7Ro"
   },
   "outputs": [],
   "source": [
    "metrics_dict = {\n",
    "    'val_acc': max(history.history['val_accuracy']),\n",
    "    'val_loss': min(history.history['val_loss']),\n",
    "    'train_acc': max(history.history['accuracy']),\n",
    "    'train_loss': min(history.history['loss'])    \n",
    "}\n",
    "mlf_run.log_metrics(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3l88LaAw7Rp"
   },
   "outputs": [],
   "source": [
    "## summarize history for loss\n",
    "## Plotting the loss with the number of iterations\n",
    "plt.semilogy(history.history['loss'])\n",
    "plt.semilogy(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84rhspVnw7Rp"
   },
   "outputs": [],
   "source": [
    "## Plotting the error with the number of iterations\n",
    "## With each iteration the error reduces smoothly\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_dk895Qw7Rp"
   },
   "outputs": [],
   "source": [
    "## read test file\n",
    "test = pd.read_csv('https://s3.amazonaws.com/hackerday.datascience/118/test.csv')\n",
    "index = test.pop('id')\n",
    "\n",
    "## we need to perform the same transformations from the training set to the test set\n",
    "test = preprocessing.MinMaxScaler().fit(test).transform(test)\n",
    "test = StandardScaler().fit(test).transform(test)\n",
    "yPred = model.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dC9APS4Qw7Rp"
   },
   "outputs": [],
   "source": [
    "yPred = pd.DataFrame(yPred,index=index,columns=sort(parent_data.species.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_TWuQqZw7Rp"
   },
   "outputs": [],
   "source": [
    "yPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myK7bhGiw7Rp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Plant_Series_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
